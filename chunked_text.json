{
    "02 - Foundations.pdf": [
        "DS 4300 Large Scale Information Storage and Retrieval Foundations Mark Fontenot, PhD Northeastern University Searching ● Searching is the most common operation performed by a database system ● In SQL, the SELECT statement is arguably the most versatile / complex. ● Baseline for efﬁciency is Linear Search ○ Start at the beginning of a list and proceed element by element until: ■ You ﬁnd what you’re looking for ■ You get to the last element and haven’t found it 2 Searching ● Record - A collection of values for attributes of a single entity instance; a row of a table ● Collection - a set of records of the same entity type; a table ○ Trivially, stored in some sequential order like a list ● Search Key - A value for an attribute from the entity type ○ Could be >= 1 attribute 3 Lists of Records ● If each record takes up x bytes of memory, then for n records, we need n*x bytes of memory. ● Contiguously Allocated List ○ All n*x bytes are allocated as a single “chunk” of memory ● Linked List ○ Each record needs x bytes + additional space for 1 or 2 memory addresses ○ Individual records are linked together in a type of chain using memory addresses 4 Contiguous vs Linked 6 Records Contiguously Allocated - Array front back Extra storage for a memory address 6 Records Linked by memory addresses - Linked List 5 ● Arrays are faster for random access, but slow for Pros and Cons inserting anywhere but the end records: records: Insert after 2nd record ● Linked Lists are faster for inserting anywhere in the 5 records had to be moved to make space list, but slower for random access Insert after 2nd record 6 - Arrays fast for random access - - slow for random insertions - Linked Lists - slow for random access - fast for random insertions Observations: 7 ● Input: array of values in sorted order, target value ● Output: the location (index) of where target is located or some value indicating target was not found Binary Search def binary_search(arr, target) left, right = 0, len(arr) - 1 while left <= right: mid = (left + right) // 2 if arr[mid] == target: return mid elif arr[mid] < target: left = mid + 1 else: right = mid - 1 return -1 left right A C G M P R Z target = A mid Since target < arr[mid], we reset right to mid - 1. left right A C G M P R Z target = A mid 8 Time Complexity ● Linear Search ○ Best case: target is found at the ﬁrst element; only 1 comparison ○ Worst case: target is not in the array; n comparisons ○ Therefore, in the worst case, linear search is O(n) time complexity. ● Binary Search ○ Best case: target is found at mid; 1 comparison (inside the loop) ○ Worst case: target is not in",
        "element; only 1 comparison ○ Worst case: target is not in the array; n comparisons ○ Therefore, in the worst case, linear search is O(n) time complexity. ● Binary Search ○ Best case: target is found at mid; 1 comparison (inside the loop) ○ Worst case: target is not in the array; log2 n comparisons ○ Therefore, in the worst case, binary search is O(log2n) time complexity. 9 Back to Database Searching ● Assume data is stored on disk by column id’s value ● Searching for a speciﬁc id = fast. ● But what if we want to search for a speciﬁc specialVal? ○ Only option is linear scan of that column ● Can’t store data on disk sorted by both id and specialVal (at the same time) ○ data would have to be duplicated → space inefﬁcient 10 Back to Database Searching ● Assume data is stored on disk by column id’s value ● Searching for a speciﬁc id = fast. ● But what if we want to search for a speciﬁc We need an external data structure specialVal? to support faster searching by ○ Only option is linear scan of that column specialVal than a linear scan. ● Can’t store data on disk sorted by both id and specialVal (at the same time) ○ data would have to be duplicated → space inefﬁcient 11 What do we have in our arsenal? 1) An array of tuples (specialVal, rowNumber) sorted by specialVal a) We could use Binary Search to quickly locate a particular specialVal and ﬁnd its corresponding row in the table b) But, every insert into the table would be like inserting into a sorted array - slow… 2) A linked list of tuples (specialVal, rowNumber) sorted by specialVal a) b) But inserting into the table would theoretically be quick to also add to searching for a specialVal would be slow - linear scan required the list. 12 Something with Fast Insert and Fast Search? - Binary Search Tree - a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent. Image from: https://courses.grainger.illinois.edu/cs225/sp2019/notes/bst/ 13 To the Board! 14"
    ],
    "03 - Moving Beyond the Relational Model.pdf": [
        "DS 4300 Moving Beyond the Relational Model Mark Fontenot, PhD Northeastern University Beneﬁts of the Relational Model - (Mostly) Standard Data Model and Query Language - ACID Compliance (more on this in a second) - Atomicity, Consistency, Isolation, Durability - Works well will highly structured data - Can handle large amounts of data - Well understood, lots of tooling, lots of experience 2 Relational Database Performance Many ways that a RDBMS increases efﬁciency: - indexing (the topic we focused on) - directly controlling storage - column oriented storage vs row oriented storage - query optimization - caching/prefetching - materialized views - precompiled stored procedures - data replication and partitioning 3 Transaction Processing - Transaction - a sequence of one or more of the CRUD operations performed as a single, logical unit of work - Either the entire sequence succeeds (COMMIT) - OR the entire sequence fails (ROLLBACK or ABORT) - Help ensure - Data Integrity - Error Recovery - Concurrency Control - Reliable Data Storage - Simpliﬁed Error Handling 4 ACID Properties - Atomicity - transaction is treated as an atomic unit - it is fully executed or no parts of it are executed - Consistency - a transaction takes a database from one consistent state to another consistent state - consistent state - all data meets integrity constraints 5 ACID Properties - Isolation - Two transactions T1 and T2 are being executed at the same - - time but cannot affect each other If both T1 and T2 are reading the data - no problem If T1 is reading the same data that T2 may be writing, can result in: - Dirty Read - Non-repeatable Read - Phantom Reads 6 Isolation: Dirty Read Dirty Read - a transaction T1 is able to read a row that has been modiﬁed by another transaction T2 that hasn’t yet executed a COMMIT Figure from: https://www.mybluelinux.com/relational-databases-explained/ 7 Isolation: Non-Repeatable Read Non-repeatable Read - two queries in a single transaction T1 execute a SELECT but get different values because another transaction T2 has changed data and COMMITTED Figure from: https://www.mybluelinux.com/relational-databases-explained/ 8 Isolation: Phantom Reads Phantom Reads - when a transaction T1 is running and another transaction T2 adds or deletes rows from the set T1 is using Figure from: https://www.mybluelinux.com/relational-databases-explained/ 9 Example Transaction - Transfer $$ DELIMITER // CREATE PROCEDURE transfer( IN sender_id INT, IN receiver_id INT, IN amount DECIMAL(10,2) ) BEGIN DECLARE rollback_message VARCHAR(255) DEFAULT 'Transaction rolled back: Insufficient funds'; DECLARE commit_message VARCHAR(255) DEFAULT 'Transaction committed successfully'; -- Start the transaction START TRANSACTION; -- Attempt to debit money from account 1 UPDATE accounts SET balance = balance - amount WHERE account_id = sender_id; -- Attempt to credit money to account 2 UPDATE accounts SET balance = balance + amount WHERE account_id = receiver_id; -- Continued Next Slide 10 Example Transaction - Transfer $$ -- Continued from previous slide -- Check if there are sufficient funds in account 1 -- Simulate a condition where there are insufficient funds IF (SELECT balance FROM accounts WHERE",
        "SET balance = balance + amount WHERE account_id = receiver_id; -- Continued Next Slide 10 Example Transaction - Transfer $$ -- Continued from previous slide -- Check if there are sufficient funds in account 1 -- Simulate a condition where there are insufficient funds IF (SELECT balance FROM accounts WHERE account_id = sender_id) < 0 THEN -- Roll back the transaction if there are insufficient funds ROLLBACK; SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = rollback_message; ELSE -- Log the transactions if there are sufficient funds INSERT INTO transactions (account_id, amount, transaction_type) -- 45000 is unhandled, user-defined error VALUES (sender_id, -amount, 'WITHDRAWAL'); INSERT INTO transactions (account_id, amount, transaction_type) VALUES (receiver_id, amount, 'DEPOSIT'); -- Commit the transaction COMMIT; SELECT commit_message AS 'Result'; END IF; END // DELIMITER ; 11 ACID Properties - Durability - Once a transaction is completed and committed successfully, its changes are permanent. - Even in the event of a system failure, committed transactions are preserved - For more info on Transactions, see: - Kleppmann Book Chapter 7 12 But … Relational Databases may not be the solution to all problems… - sometimes, schemas evolve over time - not all apps may need the full strength of ACID compliance - - a lot of data is semi-structured or unstructured (JSON, XML, joins can be expensive etc) - Horizontal scaling presents challenges - some apps need something more performant (real time, low latency systems) 13 Conventional Wisdom: Scale vertically (up, with bigger, more powerful systems) until the demands of high-availability make it necessary to scale out with some type of distributed computing model But why? Scaling up is easier - no need to really modify your architecture. But there are practical and ﬁnancial limits However: There are modern systems that make horizontal scaling less problematic. Scalability - Up or Out? 14 So what? Distributed Data when Scaling Out A distributed system is “a collection of independent computers that appear to its users as one computer.” -Andrew Tennenbaum Characteristics of Distributed Systems: - computers operate concurrently - computers fail independently - no shared global clock 15 Distributed Storage - 2 Directions Single Main Node 16 Distributed Data Stores - Data is stored on > 1 node, typically replicated i.e. each block of data is available on N nodes - - Distributed databases can be relational or non-relational - MySQL and PostgreSQL support replication and sharding - CockroachDB - new player on the scene - Many NoSQL systems support one or both models - But remember: Network partitioning is inevitable! - network failures, system failures - Overall system needs to be Partition Tolerant - System can keep running even w/ network partition 17 The CAP Theorem 18 The CAP Theorem The CAP Theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees: - Consistency - Every read receives the most recent write or error thrown - Availability - Every request receives a (non-error) response - but no guarantee that the",
        "states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees: - Consistency - Every read receives the most recent write or error thrown - Availability - Every request receives a (non-error) response - but no guarantee that the response contains the most recent write - Partition Tolerance - The system can continue to operate despite arbitrary network issues. 19 CAP Theorem - Database View - Consistency*: Every user of the DB has an identical view of the data at any given instant - Availability: In the event of a failure, the database remains operational - Partition Tolerance: The database can maintain operations in the event of the network’s failing between two segments of the distributed system * Note, the definition of Consistency in CAP is different from that of ACID. Reference: https://alperenbayramoglu.com/posts/understanding-cap-theorem/ 20 CAP Theorem - Database View - Consistency + Availability: System always responds with the latest data and every request gets a response, but may not be able to deal with network issues - Consistency + Partition Tolerance: If system responds with data from a distributed store, it is always the latest, else data request is dropped. - Availability + Partition Tolerance: System always sends are responds based on distributed store, but may not be the absolute latest data. Reference: https://alperenbayramoglu.com/posts/understanding-cap-theorem/ 21 CAP in Reality What it is really saying: - If you cannot limit the number of faults, requests can be directed to any server, and you insist on serving every request, then you cannot possibly be consistent. But it is interpreted as: - You must always give up something: consistency, availability, or tolerance to failure. 22 ?? 23"
    ],
    "05 - NoSQL Intro + KV DBs.pdf": [
        "DS 4300 NoSQL & KV DBs Mark Fontenot, PhD Northeastern University Some material used with permission from Dr. Rachlin, with thanks! Distributed DBs and ACID - Pessimistic Concurrency ● ACID transactions ○ Focuses on “data safety” ○ considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions ■ IOW, it assumes that if something can go wrong, it will. ○ Conﬂicts are prevented by locking resources until a transaction is complete (there are both read and write locks) ○ Write Lock Analogy → borrowing a book from a library… If you have it, no one else can. See https://www.freecodecamp.org/news/how-databases-guarantee-isolation for more for a deeper dive. 2 Optimistic Concurrency ● Transactions do not obtain locks on data when they read or write ● Optimistic because it assumes conﬂicts are unlikely to occur ○ Even if there is a conﬂict, everything will still be OK. ● But how? ○ Add last update timestamp and version number columns to every table… read them when changing. THEN, check at the end of transaction to see if any other transaction has caused them to be modiﬁed. 3 Optimistic Concurrency ● Low Conﬂict Systems (backups, analytical dbs, etc.) ○ Read heavy systems ○ the conﬂicts that arise can be handled by rolling back and re-running a transaction that notices a conﬂict. ○ So, optimistic concurrency works well - allows for higher concurrency ● High Conﬂict Systems ○ rolling back and rerunning transactions that encounter a conﬂict → less efﬁcient ○ So, a locking scheme (pessimistic model) might be preferable 4 NoSQL - “NoSQL” ﬁrst used in 1998 by Carlo Strozzi to describe his relational database system that did not use SQL. - More common, modern meaning is “Not Only SQL” - But, sometimes thought of as non-relational DBs - Idea originally developed, in part, as a response to processing unstructured web-based data. https://www.dataversity.net/a-brief-history-of-non-relational-databases/ 5 CAP Theorem Review You can have 2, but not 3, of the following: - Consistency*: Every user of the DB has an identical view of the data at any given instant - Availability: In the event of a failure, the database system remains operational - Partition Tolerance: The database can maintain operations in the event of the network’s failing between two segments of the distributed system * Note, the definition of Consistency in CAP is different from that of ACID. Reference: https://alperenbayramoglu.com/posts/understanding-cap-theorem/ 6 CAP Theorem Review - Consistency + Availability: System always responds with the latest data and every request gets a response, but may not be able to deal with network partitions - Consistency + Partition Tolerance: If system responds with data from the distrib. system, it is always the latest, else data request is dropped. - Availability + Partition Tolerance: System always sends are responds based on distributed store, but may not be the absolute latest data. Reference: https://alperenbayramoglu.com/posts/understanding-cap-theorem/ 7 ACID Alternative for Distrib Systems - BASE ● Basically Available ○ Guarantees the availability of the data (per CAP), but response can be",
        "is dropped. - Availability + Partition Tolerance: System always sends are responds based on distributed store, but may not be the absolute latest data. Reference: https://alperenbayramoglu.com/posts/understanding-cap-theorem/ 7 ACID Alternative for Distrib Systems - BASE ● Basically Available ○ Guarantees the availability of the data (per CAP), but response can be “failure”/“unreliable” because the data is in an inconsistent or changing state ○ System appears to work most of the time 8 ACID Alternative for Distrib Systems - BASE ● Soft State - The state of the system could change over time, even w/o input. Changes could be result of eventual consistency. ○ Data stores don’t have to be write-consistent ○ Replicas don’t have to be mutually consistent 9 ACID Alternative for Distrib Systems - BASE ● Eventual Consistency - The system will eventually become consistent ○ All writes will eventually stop so all nodes/replicas can be updated 10 Categories of NoSQL DBs - Review 11 First Up → Key-Value Databases 12 Key Value Stores key = value - Key-value stores are designed around: - the data model is extremely simple simplicity - - comparatively, tables in a RDBMS are very complex. lends itself to simple CRUD ops and API creation - 13 Key Value Stores key = value - Key-value stores are designed around: - speed - usually deployed as in-memory DB - retrieving a value given its key is typically a O(1) op b/c hash tables or similar data structs used under the hood - no concept of complex queries or joins… they slow things down 14 Key Value Stores key = value - Key-value stores are designed around: - scalability - Horizontal Scaling is simple - add more nodes - Typically concerned with eventual consistency, meaning in a distributed environment, the only guarantee is that all nodes will eventually converge on the same value. 15 KV DS Use Cases - EDA/Experimentation Results Store - store intermediate results from data preprocessing and EDA - store experiment or testing (A/B) results w/o prod db - Feature Store - store frequently accessed feature → low-latency retrieval for model training and prediction - Model Monitoring - store key metrics about performance of model, for example, in real-time inferencing. 16 KV SWE Use Cases - Storing Session Information - everything about the current session can be stored via a single PUT or POST and retrieved with a single GET …. VERY Fast - User Proﬁles & Preferences - User info could be obtained with a single GET operation… language, TZ, product or UI preferences - Shopping Cart Data - Cart data is tied to the user - needs to be available across browsers, machines, sessions - Caching Layer: - In front of a disk-based database 17 - Redis (Remote Directory Server) Redis DB - Open source, in-memory database - Sometimes called a data structure store - Primarily a KV store, but can be used with other models: Graph, Spatial, Full Text Search, Vector, Time Series - From db-engines.com Ranking of KV Stores: 18",
        "disk-based database 17 - Redis (Remote Directory Server) Redis DB - Open source, in-memory database - Sometimes called a data structure store - Primarily a KV store, but can be used with other models: Graph, Spatial, Full Text Search, Vector, Time Series - From db-engines.com Ranking of KV Stores: 18 Redis - It is considered an in-memory database system, but… - Supports durability of data by: a) essentially saving snapshots to disk at speciﬁc intervals or b) append-only ﬁle which is a journal of changes that can be used for roll-forward if there is a failure - Originally developed in 2009 in C++ - Can be very fast … > 100,000 SET ops / second - Rich collection of commands - Does NOT handle complex data. No secondary indexes. Only supports lookup by Key. 19 Redis Data Types Keys: - usually strings but can be any binary sequence Values: - Strings - Lists (linked lists) - Sets (unique unsorted string elements) - Sorted Sets - Hashes (string → string) - Geospatial data 20 Setting Up Redis in Docker - In Docker Desktop, search for Redis. - Pull/Run the latest image (see above) - Optional Settings: add 6379 to Ports to expose that port so we can connect to it. - Normally, you would not expose the Redis port for security reasons - - Notice, we didn’t set a password… If you did this in a prod environment, major security hole. 21 - File > New > Data Source > Redis - Give the Data Source a Name - Make sure the port is 6379 - Test the connection ✅ Connecting from DataGrip 22 Redis Database and Interaction - Redis provides 16 databases by default - They are numbered 0 to 15 - There is no other name associated - Direct interaction with Redis is through a set of commands related to setting and getting k/v pairs (and variations) - Many language libraries available as well. 23 Foundation Data Type - String - Sequence of bytes - text, serialized objects, bin arrays - Simplest data type - Maps a string to another string - Use Cases: - caching frequently accessed HTML/CSS/JS fragments - conﬁg settings, user settings info, token management - counting web page/app screen views OR rate limiting 24 Some Initial Basic Commands - SET /path/to/resource 0 SET user:1 “John Doe” GET /path/to/resource EXISTS user:1 DEL user:1 KEYS user* - SELECT 5 - select a different database 25 Some Basic Commands - SET someValue 0 INCR someValue #increment by 1 INCRBY someValue 10 #increment by 10 DECR someValue #decrement by 1 DECRBY someValue 5 #decrement by 5 - INCR parses the value as int and increments (or adds to value) - SETNX key value - only sets value to key if key does not already exist 26 Hash Type - Value of KV entry is a collection of ﬁeld-value pairs - Use Cases: - Can be used to represent basic objects/structures - number of ﬁeld/value pairs per hash is 2^32-1",
        "- SETNX key value - only sets value to key if key does not already exist 26 Hash Type - Value of KV entry is a collection of ﬁeld-value pairs - Use Cases: - Can be used to represent basic objects/structures - number of ﬁeld/value pairs per hash is 2^32-1 - practical limit: available system resources (e.g. memory) - Session information management - User/Event tracking (could include TTL) - Active Session Tracking (all sessions under one hash key) 27 Hash Commands HSET bike:1 model Demios brand Ergonom price 1971 HGET bike:1 model HGET bike:1 price HGETALL bike:1 HMGET bike:1 model price weight HINCRBY bike:1 price 100 What is returned? 28 List Type - Value of KV Pair is linked lists of string values - Use Cases: implementation of stacks and queues - - queue management & message passing queues (producer/consumer model) logging systems (easy to keep in chronological order) - - build social media streams/feeds - message history in a chat application - batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 front 10 Linked Lists Crash Course back nil - Sequential data structure of linked nodes (instead of contiguously allocated memory) - Each node points to the next element of the list (except the last one - points to nil/null) - O(1) to insert new value at front or insert new value at end 30 List Commands - Queue Queue-like Ops LPUSH bikes:repairs bike:1 LPUSH bikes:repairs bike:2 RPOP bikes:repairs RPOP biles:repairs 31 List Commands - Stack Stack-like Ops LPUSH bikes:repairs bike:1 LPUSH bikes:repairs bike:2 LPOP bikes:repairs LPOP biles:repairs 32 Other List Ops LLEN mylist List Commands - Others LPUSH mylist “one” LPUSH mylist “two” LPUSH mylist “three” LRANGE <key> <start> <stop> LRANGE mylist 0 3 LRANGE mylist 0 0 LRANGE mylist -2 -1 33 JSON Type - Full support of the JSON standard - Uses JSONPath syntax for parsing/navigating a JSON document - Internally, stored in binary in a tree-structure → fast access to sub elements 34 Set Type - Unordered collection of unique strings (members) - Use Cases: track unique items (IP addresses visiting a site, page, screen) - - primitive relation (set of all students in DS4300) - access control lists for users and permission structures - social network friends lists and/or group membership - Supports set operations!! 35 SADD ds4300 “Mark” SADD ds4300 “Sam” SADD cs3200 “Nick” SADD cs3200 “Sam” SISMEMBER ds4300 “Mark” SISMEMBER ds4300 “Nick” SCARD ds4300 Set Commands 36 SADD ds4300 “Mark” SADD ds4300 “Sam” SADD cs3200 “Nick” SADD cs3200 “Sam” SCARD ds4300 SINTER ds4300 cs3200 SDIFF ds4300 cs3200 SREM ds4300 “Mark” SRANDMEMBER ds4300 Set Commands 37 ?? 38"
    ],
    "06 - Redis + Python.pdf": [
        "DS 4300 Redis + Python Mark Fontenot, PhD Northeastern University Redis-py - Redis-py is the standard client for Python. - Maintained by the Redis Company itself - GitHub Repo: redis/redis-py - In your 4300 Conda Environment: pip install redis 2 Connecting to the Server import redis redis_client = redis.Redis(host=’localhost’, port=6379, db=2, decode_responses=True) - For your Docker deployment, host could be localhost or 127.0.0.1 - Port is the port mapping given when you created the container (probably the default 6379) - db is the database 0-15 you want to connect to - decode_responses → data comes back from server as bytes. Setting this true converter them (decodes) to strings. 3 Redis Command List - Full List > here < - Use Filter to get to command for the particular data structure you’re targeting (list, hash, set, etc.) - Redis.py Documentation > here < - The next slides are not meant to be an exhaustive list of commands, only some highlights. Check the documentation for a complete list. 4 String Commands # r represents the Redis client object r.set(‘clickCount:/abc’, 0) val = r.get(‘clickCount:/abc’) r.incr(‘clickCount:/abc’) ret_val = r.get(‘clickCount:/abc’) print(f’click count = {ret_val}’) 5 String Commands - 2 # r represents the Redis client object redis_client.mset({'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}) print(redis_client.mget('key1', 'key2', 'key3')) # returns as list [‘val1’, ‘val2’, ‘val3’] 6 String Commands - 3 - set(), mset(), setex(), msetnx(), setnx() - get(), mget(), getex(), getdel() - incr(), decr(), incrby(), decrby() - strlen(), append() 7 List Commands - 1 # create list: key = ‘names’ # values = [‘mark’, ‘sam’, ‘nick’] redis_client.rpush('names', 'mark', 'sam', 'nick') # prints [‘mark’, ‘sam’, ‘nick’] print(redis_client.lrange('names', 0, -1)) 8 List Commands - 2 - lpush(), lpop(), lset(), lrem() - rpush(), rpop() - lrange(), llen(), lpos() - Other commands include moving elements between lists, popping from multiple lists at the same time, etc. 9 Hash Commands - 1 redis_client.hset('user-session:123', mapping={'first': 'Sam', 'last': 'Uelle', 'company': 'Redis', 'age': 30 }) # prints: #{'name': 'Sam', 'surname': 'Uelle', 'company': 'Redis', 'age': '30'} print(redis_client.hgetall('user-session:123')) 10 Hash Commands - 2 - hset(), hget(), hgetall() - hkeys() - hdel(), hexists(), hlen(), hstrlen() 11 Redis Pipelines - Helps avoid multiple related calls to the server → less network overhead r = redis.Redis(decode_responses=True) pipe = r.pipeline() for i in range(5): pipe.set(f\"seat:{i}\", f\"#{i}\") set_5_result = pipe.execute() print(set_5_result) # >>> [True, True, True, True, True] pipe = r.pipeline() # \"Chain\" pipeline commands together. get_3_result = pipe.get(\"seat:0\").get(\"seat:3\").get(\"seat:4\").execute() print(get_3_result) # >>> ['#0', '#3', '#4'] 12 Redis in Context 13 Redis in ML - Simpliﬁed Example Source: https://www.featureform.com/post/feature-stores-explained-the-three-common-architectures 14 Redis in DS/ML Source: https://madewithml.com/courses/mlops/feature-store/ 15"
    ],
    "07 - Document DBs and Mongo.pdf": [
        "DS 4300 Document Databases & MongoDB Mark Fontenot, PhD Northeastern University Some material used with permission from Dr. Rachlin, with thanks! A Document Database is a non-relational database that stores data as structured documents, usually in JSON. They are designed to be simple, ﬂexible, and scalable. Document Database 2 What is JSON? ● JSON (JavaScript Object Notation) ○ a lightweight data-interchange format ○ It is easy for humans to read and write. ○ It is easy for machines to parse and generate. ● JSON is built on two structures: ○ A collection of name/value pairs. In various languages, this is operationalized as an object, record, struct, dictionary, hash table, keyed list, or associative array. ○ An ordered list of values. In most languages, this is operationalized as an array, vector, list, or sequence. ● These are two universal data structures supported by virtually all modern programming languages ○ Thus, JSON makes a great data interchange format. 3 JSON Syntax https://www.json.org/json-en.html 4 Binary JSON? BSON - BSON → Binary JSON - binary-encoded serialization of a JSON-like document structure - - - Traversable - designed to be easily traversed, which is vitally important to a supports extended types not part of basic JSON (e.g. Date, BinaryData, etc) Lightweight - keep space overhead to a minimum document DB - Efﬁcient - encoding and decoding must be efﬁcient - Supported by many modern programming languages 5 XML (eXtensible Markup Language) ● Precursor to JSON as data exchange format ● XML + CSS → web pages that separated content and formatting ● Structurally similar to HTML, but tag set is extensible 6 XML-Related Tools/Technologies - Xpath - a syntax for retrieving speciﬁc elements from an XML doc - Xquery - a query language for interrogating XML documents; the SQL of XML - DTD - Document Type Deﬁnition - a language for describing the allowed structure of an XML document - XSLT - eXtensible Stylesheet Language Transformation - tool to transform XML into other formats, including non-XML formats such as HTML. 7 Why Document Databases? - Document databases address the impedance mismatch problem between object persistence in OO systems and how relational DBs structure data. - OO Programming → Inheritance and Composition of types. - How do we save a complex object to a relational database? We basically have to deconstruct it. - The structure of a document is self-describing. - They are well-aligned with apps that use JSON/XML as a transport layer 8 MongoDB 9 MongoDB - Started in 2007 after Doubleclick was acquired by Google, and 3 of its veterans realized the limitations of relational databases for serving > 400,000 ads per second - MongoDB was short for Humongous Database - MongoDB Atlas released in 2016 → documentdb as a service https://www.mongodb.com/company/our-story 10 MongoDB Structure Database Collection A Collection B Collection C Document 1 Document 1 Document 1 Document 2 Document 2 Document 2 Document 3 Document 3 Document 3 11 MongoDB Documents - No predeﬁned schema for documents is needed - Every document",
        "in 2016 → documentdb as a service https://www.mongodb.com/company/our-story 10 MongoDB Structure Database Collection A Collection B Collection C Document 1 Document 1 Document 1 Document 2 Document 2 Document 2 Document 3 Document 3 Document 3 11 MongoDB Documents - No predeﬁned schema for documents is needed - Every document in a collection could have different data/schema 12 Relational vs Mongo/Document DB RDBMS Database Table/View Row Column Index Join MongoDB Database Collection Document Field Index Embedded Document Foreign Key Reference 13 MongoDB Features - Rich Query Support - robust support for all CRUD ops - Indexing - supports primary and secondary indices on document ﬁelds - Replication - supports replica sets with automatic failover - Load balancing built in 14 MongoDB Versions ● MongoDB Atlas ○ Fully managed MongoDB service in the cloud (DBaaS) ● MongoDB Enterprise ○ Subscription-based, self-managed version of MongoDB ● MongoDB Community ○ source-available, free-to-use, self-managed 15 Interacting with MongoDB ● mongosh → MongoDB Shell ○ CLI tool for interacting with a MongoDB instance ● MongoDB Compass ○ free, open-source GUI to work with a MongoDB database ● DataGrip and other 3rd Party Tools ● Every major language has a library to interface with MongoDB ○ PyMongo (Python), Mongoose (JavaScript/node), … 16 Mongodb Community Edition in Docker - Create a container - Map host:container port 27017 - Give initial username and password for superuser E D 17 - GUI Tool for interacting with MongoDB instance - Download and install from > here <. MongoDB Compass 18 Load MFlix Sample Data Set - In Compass, create a new Database named mﬂix - Download mﬂix sample dataset and unzip it - Import JSON ﬁles for users, theaters, movies, and comments into new collections in the mﬂix database 19 Creating a Database and Collection To Create a new DB: To Create a new Collection: mﬂix users 20 mongosh - Mongo Shell - ﬁnd(...) is like SELECT collection.find({ ____ }, { ____ }) ﬁlters projections 21 - SELECT * FROM users; use mflix db.users.find() mongosh - ﬁnd() 22 - SELECT * FROM users WHERE name = “Davos Seaworth”; mongosh - ﬁnd() ﬁlter db.users.find({\"name\": \"Davos Seaworth\"}) 23 - SELECT * FROM movies WHERE rated in (\"PG\", \"PG-13\") mongosh - ﬁnd() db.movies.find({rated: {$in:[ \"PG\", \"PG-13\" ]}}) 24 mongosh - ﬁnd() - Return movies which were released in Mexico and have an IMDB rating of at least 7 db.movies.find( { \"countries\": \"Mexico\", \"imdb.rating\": { $gte: 7 } } ) 25 - Return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of Drama mongosh - ﬁnd() db.movies.find( { “year”: 2010, $or: [ { \"awards.wins\": { $gte: 5 } }, { “genres”: \"Drama\" } ] }) 26 Comparison Operators 27 mongosh - countDocuments() - How many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of Drama db.movies.countDocuments( { “year”: 2010, $or: [ { \"awards.wins\": { $gte: 5 } }, {",
        "\"Drama\" } ] }) 26 Comparison Operators 27 mongosh - countDocuments() - How many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of Drama db.movies.countDocuments( { “year”: 2010, $or: [ { \"awards.wins\": { $gte: 5 } }, { “genres”: \"Drama\" } ] }) 28 - Return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of Drama mongosh - project db.movies.countDocuments( { “year”: 2010, $or: [ { \"awards.wins\": { $gte: 5 } }, { “genres”: \"Drama\" } ] }, {“name”: 1, “_id”: 0} ) 1 = return; 0 = don’t return 29 PyMongo 30 ● PyMongo is a Python library for interfacing with MongoDB instances PyMongo from pymongo import MongoClient client = MongoClient( ‘mongodb://user_name:pw@localhost:27017’ ) 31 Getting a Database and Collection from pymongo import MongoClient client = MongoClient( ‘mongodb://user_name:pw@localhost:27017’ ) db = client[‘ds4300’] collection = db[‘myCollection’] 32 Inserting a Single Document db = client[‘ds4300’] collection = db[‘myCollection’] post = { “author”: “Mark”, “text”: “MongoDB is Cool!”, “tags”: [“mongodb”, “python”] } post_id = collection.insert_one(post).inserted_id print(post_id) 33 Count Documents in Collection - SELECT count(*) FROM collection demodb.collection.count_documents({}) 34 ?? 35"
    ],
    "08 - PyMongo.pdf": [
        "DS 4300 MongoDB + PyMongo Mark Fontenot, PhD Northeastern University ● PyMongo is a Python library for interfacing with MongoDB instances PyMongo from pymongo import MongoClient client = MongoClient( ‘mongodb://user_name:pw@localhost:27017’ ) 2 Getting a Database and Collection from pymongo import MongoClient client = MongoClient( ‘mongodb://user_name:pw@localhost:27017’ ) db = client[‘ds4300’] # or client.ds4300 collection = db[‘myCollection’] #or db.myCollection 3 Inserting a Single Document db = client[‘ds4300’] collection = db[‘myCollection’] post = { “author”: “Mark”, “text”: “MongoDB is Cool!”, “tags”: [“mongodb”, “python”] } post_id = collection.insert_one(post).inserted_id print(post_id) 4 Find all Movies from 2000 from bson.json_util import dumps # Find all movies released in 2000 movies_2000 = db.movies.find({\"year\": 2000}) # Print results print(dumps(movies_2000, indent = 2)) 5 Jupyter Time - Activate your DS4300 conda or venv python environment - Install pymongo with pip install pymongo - Install Jupyter Lab in you python environment - pip install jupyterlab - Download and unzip > this < zip ﬁle - contains 2 Jupyter - Notebooks In terminal, navigate to the folder where you unzipped the ﬁles, and run jupyter lab 6 ?? 7"
    ],
    "09 - Introduction to Graph Data Model.pdf": [
        "DS 4300 Introduction to the Graph Data Model Mark Fontenot, PhD Northeastern University Material referenced from Graph Algorithms - Practical Examples in Apache Spark and Neo4j by Needham and Hodler (O’Reilly Press, 2019) What is a Graph Database - Data model based on the graph data structure - Composed of nodes and edges - edges connect nodes - each is uniquely identiﬁed - each can contain properties (e.g. name, occupation, etc) - supports queries based on graph-oriented operations traversals - - shortest path lots of others - 2 Where do Graphs Show up? - Social Networks - yes… things like Instagram, - but also… modeling social interactions in ﬁelds like psychology and sociology - The Web - it is just a big graph of “pages” (nodes) connected by hyperlinks (edges) - Chemical and biological data - systems biology, genetics, etc. - interaction relationships in chemistry 3 Basics of Graphs and Graph Theory 4 What is a graph? Labeled Property Graph - Composed of a set of node (vertex) objects and relationship (edge) objects - Labels are used to mark a node as part of a group - Properties are attributes (think KV pairs) and can exist on nodes and relationships - Nodes with no associated relationships are OK. Edges not connected to nodes are not permitted. 5 Example 2 Labels: - person - car 4 relationship types: - Drives - Owns - Lives_with - Married_to Properties 6 A path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated. Paths 1 2 3 6 5 4 Ex: 1 → 2 → 6 → 5 Not a path: 1 → 2 → 6 → 2 → 3 7 Flavors of Graphs Connected (vs. Disconnected) – there is a path between any two nodes in the graph Weighted (vs. Unweighted) – edge has a weight property (important for some algorithms) Directed (vs. Undirected) – relationships (edges) deﬁne a start and end node Acyclic (vs. Cyclic) – Graph contains no cycles 8 Connected vs. Disconnected 9 Weighted vs. Unweighted 10 Directed vs. Undirected 11 Cyclic vs Acyclic 12 Sparse vs. Dense 13 Trees 14 Types of Graph Algorithms - Pathﬁnding - Pathﬁnding - ﬁnding the shortest path between two nodes, if one exists, is probably the most common operation “shortest” means fewest edges or lowest weight - - Average Shortest Path can be used to monitor efﬁciency and resiliency of networks. - Minimum spanning tree, cycle detection, max/min ﬂow… are other types of pathﬁnding 15 BFS vs DFS 16 Shortest Path 17 Types of Graph Algorithms - Centrality & Community Detection - Centrality - determining which nodes are “more important” in a network compared to other nodes - EX: Social Network Inﬂuencers? - Community Detection - evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 Centrality 19 Some Famous Graph Algorithms - Dijkstra’s Algorithm - single-source shortest path algo for positively weighted graphs - A* Algorithm - Similar",
        "nodes - EX: Social Network Inﬂuencers? - Community Detection - evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 Centrality 19 Some Famous Graph Algorithms - Dijkstra’s Algorithm - single-source shortest path algo for positively weighted graphs - A* Algorithm - Similar to Dijkstra’s with added feature of using a heuristic to guide traversal - PageRank - measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 Neo4j - A Graph Database System that supports both transactional and analytical processing of graph-based data - Relatively new class of no-sql DBs - Considered schema optional (one can be imposed) - Supports various types of indexing - ACID compliant - Supports distributed computing - Similar: Microsoft CosmoDB, Amazon Neptune 21 ?? 22"
    ],
    "10 - Neo4j.pdf": [
        "DS 4300 Neo4j Mark Fontenot, PhD Northeastern University Material referenced from Graph Algorithms - Practical Examples in Apache Spark and Neo4j by Needham and Hodler (O’Reilly Press, 2019) Neo4j - A Graph Database System that supports both transactional and analytical processing of graph-based data - Relatively new class of no-sql DBs - Considered schema optional (one can be imposed) - Supports various types of indexing - ACID compliant - Supports distributed computing - Similar: Microsoft CosmoDB, Amazon Neptune 2 Neo4j - Query Language and Plugins - Cypher - Neo4j’s graph query language created in 2011 - Goal: SQL-equivalent language for graph databases - Provides a visual way of matching patterns and relationships (nodes)-[:CONNECT_TO]->(otherNodes) - APOC Plugin - Awesome Procedures on Cypher - Add-on library that provides hundreds of procedures and functions - Graph Data Science Plugin - provides efﬁcient implementations of common graph algorithms (like the ones we talked about yesterday) 3 Neo4j in Docker Compose 4 ● Supports multi-container management. ● Set-up is declarative - using YAML docker-compose.yaml Docker Compose ﬁle ○ services ○ volumes ○ networks, etc. ● 1 command can be used to start, stop, or scale a number of services at one time. ● Provides a consistent method for producing an identical environment (no more “well… it works on my machine!) ● Interaction is mostly via command line 5 docker-compose.yaml Never put “secrets” in a docker compose ﬁle. Use .env ﬁles. services: neo4j: container_name: neo4j image: neo4j:latest ports: - 7474:7474 - 7687:7687 environment: - NEO4J_AUTH=neo4j/${NEO4J_PASSWORD} - NEO4J_apoc_export_file_enabled=true - NEO4J_apoc_import_file_enabled=true - NEO4J_apoc_import_file_use__neo4j__config=true - NEO4J_PLUGINS=[\"apoc\", \"graph-data-science\"] volumes: - ./neo4j_db/data:/data - ./neo4j_db/logs:/logs - ./neo4j_db/import:/var/lib/neo4j/import - ./neo4j_db/plugins:/plugins 6 .env Files - .env ﬁles - stores a collection of environment variables - good way to keep environment variables for different platforms separate - - - .env.local .env.dev .env.prod .env file NEO4J_PASSWORD=abc123!!! 7 Docker Compose Commands ● To test if you have Docker CLI properly installed, run: docker --version ● Major Docker Commands ○ docker compose up ○ docker compose up -d ○ docker compose down ○ docker compose start ○ docker compose stop ○ docker compose build ○ docker compose build --no-cache 8 localhost:7474 9 localhost:7474 Then login. N e o 4 j B r o w s e r https://neo4j.com/docs/browser-manual/current/visual-tour/ 10 Inserting Data by Creating Nodes CREATE (:User {name: \"Alice\", birthPlace: \"Paris\"}) CREATE (:User {name: \"Bob\", birthPlace: \"London\"}) CREATE (:User {name: \"Carol\", birthPlace: \"London\"}) CREATE (:User {name: \"Dave\", birthPlace: \"London\"}) CREATE (:User {name: \"Eve\", birthPlace: \"Rome\"}) 11 Adding an Edge with No Variable Names CREATE (:User {name: \"Alice\", birthPlace: \"Paris\"}) CREATE (:User {name: \"Bob\", birthPlace: \"London\"}) MATCH (alice:User {name:”Alice”}) MATCH (bob:User {name: “Bob”}) CREATE (alice)-[:KNOWS {since: “2022-12-01”}]->(bob) Note: Relationships are directed in neo4j. 12 Which users were born in London? Matching MATCH (usr:User {birthPlace: “London”}) RETURN usr.name, usr.birthPlace 13 Download Dataset and Move to Import Folder Clone this repo: https://github.com/PacktPublishing/Graph-Data-Science-with-Neo4j In Chapter02/data of data repo, unzip the netﬂix.zip ﬁle Copy netﬂix_titles.csv into the following folder where you put your docker compose ﬁle neo4j_db/neo4j_db/import 14 Importing Data 15 Type the following",
        "Matching MATCH (usr:User {birthPlace: “London”}) RETURN usr.name, usr.birthPlace 13 Download Dataset and Move to Import Folder Clone this repo: https://github.com/PacktPublishing/Graph-Data-Science-with-Neo4j In Chapter02/data of data repo, unzip the netﬂix.zip ﬁle Copy netﬂix_titles.csv into the following folder where you put your docker compose ﬁle neo4j_db/neo4j_db/import 14 Importing Data 15 Type the following into the Cypher Editor in Neo4j Browser Basic Data Importing LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line CREATE(:Movie { id: line.show_id, title: line.title, releaseYear: line.release_year } ) 16 Loading CSVs - General Syntax LOAD CSV [WITH HEADERS] FROM 'file:///file_in_import_folder.csv' AS line [FIELDTERMINATOR ','] // do stuffs with 'line' 17 Importing with Directors this Time LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line WITH split(line.director, \",\") as directors_list UNWIND directors_list AS director_name CREATE (:Person {name: trim(director_name)}) But this generates duplicate Person nodes (a director can direct more than 1 movie) 18 Importing with Directors Merged MATCH (p:Person) DELETE p LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line WITH split(line.director, \",\") as directors_list UNWIND directors_list AS director_name MERGE (:Person {name: director_name}) 19 Adding Edges LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line MATCH (m:Movie {id: line.show_id}) WITH m, split(line.director, \",\") as directors_list UNWIND directors_list AS director_name MATCH (p:Person {name: director_name}) CREATE (p)-[:DIRECTED]->(m) 20 Let’s check the movie titled Ray: Gut Check MATCH (m:Movie {title: \"Ray\"})<-[:DIRECTED]-(p:Person) RETURN m, p 21 ?? 22"
    ],
    "11 - AWS Intro.pdf": [
        "DS 4300 AWS Introduction Mark Fontenot, PhD Northeastern University Amazon Web Services ● Leading Cloud Platform with over 200 different services available ● Globally available via its massive networks of regions and availability zones with their massive data centers ● Based on a pay-as-you-use cost model. ○ Theoretically cheaper than renting rackspace/servers in a data center… Theoretically. 2 History of AWS ● Originally launched in 2006 with only 2 services: S3 & EC2. ● By 2010, services had expanded to include SimpleDB, Elastic Block Store, Relational Database Service, DynamoDB, CloudWatch, Simple Workﬂow, CloudFront, Availability Zones, and others. ● Amazon had competitions with big prizes to spur the adoption of AWS in its early days ● They’ve continuously innovated, always introducing new services for ops, dev, analytics, etc… (200+ services now) 3 AWS Service Categories 4 Cloud Models ● IaaS (more) - Infrastructure as a Service ○ Contains the basic services that are needed to build an IT infrastructure ● PaaS (more) - Platform as a Service ○ Remove the need for having to manage infrastructure ○ You can get right to deploying your app ● SaaS (more) - Software as a Service ○ Provide full software apps that are run and managed by another party/vendor 5 C l o u d M o d e l s https://bluexp.netapp.com/iaas 6 The Shared Responsibility Model - AWS - AWS Responsibilities (Security OF the cloud): - Security of physical infrastructure (infra) and network - keep the data centers secure, control access to them - maintain power availability, HVAC, etc. - monitor and maintain physical networking equipment and global infra/connectivity - Hypervisor & Host OSs - manage the virtualization layer used in AWS compute services - maintaining underlying host OSs for other services - Maintaining managed services - keep infra up to date and functional - maintain server software (patching, etc) 7 The Shared Responsibility Model - Client - Client Responsibilities (Security IN the cloud): - Control of Data/Content - - client controls how its data is classiﬁed, encrypted, and shared implement and enforce appropriate data-handling policies - Access Management & IAM - properly conﬁgure IAM users, roles, and policies. - enforce the Principle of Least Privilege - Manage self-hosted Apps and associated OSs - Ensure network security to its VPC - Handle compliance and governance policies and procedures 8 The AWS Global Infrastructure - Regions - distinct geographical areas - us-east-1, us-west 1, etc - Availability Zones (AZs) - each region has multiple AZs - roughly equiv to isolated data centers - Edge Locations locations for CDN and other types of caching services - - allows content to be closer to end user. 9 https://aws.amazon.com/about-aws/global-infrastructure/ 10 Compute Services - VM-based: - EC2 & EC2 Spot - Elastic Cloud Compute - Container-based: - ECS - Elastic Container Service - ECR - Elastic Container Registry - EKS - Elastic Kubernetes Service - Fargate - Serverless container service - Serverless: AWS Lambda https://aws.amazon.com/products/compute/ 11 Storage Services ● Amazon S3 - Simple Storage Service ○ Object storage",
        "EC2 Spot - Elastic Cloud Compute - Container-based: - ECS - Elastic Container Service - ECR - Elastic Container Registry - EKS - Elastic Kubernetes Service - Fargate - Serverless container service - Serverless: AWS Lambda https://aws.amazon.com/products/compute/ 11 Storage Services ● Amazon S3 - Simple Storage Service ○ Object storage in buckets; highly scalable; different storage classes ● Amazon EFS - Elastic File System ○ Simple, serverless, elastic, “set-and-forget” ﬁle system ● Amazon EBS - Elastic Block Storage ○ High-Performance block storage service ● Amazon File Cache ○ High-speed cache for datasets stored anywhere ● AWS Backup ○ Fully managed, policy-based service to automate data protection and compliance of apps on AWS https://aws.amazon.com/products/storage/ 12 Database Services ● Relational - Amazon RDS, Amazon Aurora ● Key-Value - Amazon DynamoDB ● In-Memory - Amazon MemoryDB, Amazon ElastiCache ● Document - Amazon DocumentDB (Compat with MongoDB) ● Graph - Amazon Neptune 13 Analytics Services ● Amazon Athena - Analyze petabyte scale data where it lives (S3, for example) ● Amazon EMR - Elastic MapReduce - Access Apache Spark, Hive, Presto, etc. ● AWS Glue - Discover, prepare, and integrate all your data ● Amazon Redshift - Data warehousing service ● Amazon Kinesis - real-time data streaming ● Amazon QuickSight - cloud-native BI/reporting tool 14 ML and AI Services - Amazon SageMaker fully-managed ML platform, including Jupyter NBs - - build, train, deploy ML models - AWS AI Services w/ Pre-trained Models - Amazon Comprehend - NLP - Amazon Rekognition - Image/Video analysis - Amazon Textract - Text extraction - Amazon Translate - Machine translation 15 Important Services for Data Analytics/Engineering - EC2 and Lambda - Amazon S3 - Amazon RDS and DynamoDB - AWS Glue - Amazon Athena - Amazon EMR - Amazon Redshift 16 AWS Free Tier ● Allows you to gain hands-on experience with a subset of the services for 12 months (service limitations apply as well) ○ Amazon EC2 - 750 hours/month (speciﬁc OSs and Instance Sizes) ○ Amazon S3 - 5GB (20K GETs, 2K Puts) ○ Amazon RDS - 750 hours/month of DB use (within certain limits) ○ ….. So many free services 17 ?? 18"
    ],
    "12.6. B-Trees — CS3 Data Structures & Algorithms.pdf": [
        "3/12/25, 1:18 PM 12.6. B-Trees — CS3 Data Structures & Algorithms 12.6. B-Trees 12.6.1. B-Trees This module presents the B-tree. B-trees are usually attributed to R. Bayer and E. McCreight who described the B-tree in a 1972 paper. By 1979, B-trees had replaced virtually all large-file access methods other than hashing. B-trees, or some variant of B-trees, are the standard file organization for applications requiring insertion, deletion, and key range searches. They are used to implement most modern file systems. B-trees address effectively all of the major problems encountered when implementing disk-based search trees: 1. The B-tree is shallow, in part because the tree is always height balanced (all leaf nodes are at the same level), and in part because the branching factor is quite high. So only a small number of disk blocks are accessed to reach a given record. 2. Update and search operations affect only those disk blocks on the path from the root to the leaf node containing the query record. The fewer the number of disk blocks affected during an operation, the less disk I/O is required. 3. B-trees keep related records (that is, records with similar key values) on the same disk block, which helps to minimize disk I/O on range searches. 4. B-trees guarantee that every node in the tree will be full at least to a certain minimum percentage. This improves space efficiency while reducing the typical number of disk fetches necessary during a search or update operation. A B-tree of order m is defined to have the following shape properties: The root is either a leaf or has at least two children. Each internal node, except for the root, has between ⌈m/2⌉ and m children. All leaves are at the same level in the tree, so the tree is always height balanced. The B-tree is a generalization of the 2-3 tree. Put another way, a 2-3 tree is a B-tree of order three. Normally, the size of a node in the B- tree is chosen to fill a disk block. A B-tree node implementation typically allows 100 or more children. Thus, a B-tree node is equivalent to a disk block, and a “pointer” value stored in the tree is actually the number of the block containing the child node (usually interpreted as an offset from the beginning of the corresponding disk file). In a typical application, the B-tree’s access to the disk file will be managed using a buffer pool and a block-replacement scheme such as LRU. Figure 12.6.1 shows a B-tree of order four. Each node contains up to three keys, and internal nodes have up to four children. 24 15 20 33 45 48 10 12 18 21 23 30 30 38 47 50 52 60 Figure 12.6.1: A B-tree of order four. Search in a B-tree is a generalization of search in a 2-3 tree. It is an alternating two-step process, beginning with the root node of the B- tree. 1. Perform a binary search on the records",
        "30 30 38 47 50 52 60 Figure 12.6.1: A B-tree of order four. Search in a B-tree is a generalization of search in a 2-3 tree. It is an alternating two-step process, beginning with the root node of the B- tree. 1. Perform a binary search on the records in the current node. If a record with the search key is found, then return that record. If the current node is a leaf node and the key is not found, then report an unsuccessful search. https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 1/9 3/12/25, 1:18 PM 12.6. B-Trees — CS3 Data Structures & Algorithms 2. Otherwise, follow the proper branch and repeat the process. For example, consider a search for the record with key value 47 in the tree of Figure 12.6.1. The root node is examined and the second (right) branch taken. After examining the node at level 1, the third branch is taken to the next level to arrive at the leaf node containing a record with key value 47. B-tree insertion is a generalization of 2-3 tree insertion. The first step is to find the leaf node that should contain the key to be inserted, space permitting. If there is room in this node, then insert the key. If there is not, then split the node into two and promote the middle key to the parent. If the parent becomes full, then it is split in turn, and its middle key promoted. Note that this insertion process is guaranteed to keep all nodes at least half full. For example, when we attempt to insert into a full internal node of a B-tree of order four, there will now be five children that must be dealt with. The node is split into two nodes containing two keys each, thus retaining the B-tree property. The middle of the five children is promoted to its parent. 12.6.1.1. B+ Trees The previous section mentioned that B-trees are universally used to implement large-scale disk-based systems. Actually, the B-tree as described in the previous section is almost never implemented. What is most commonly implemented is a variant of the B-tree, called the B+ tree. When greater efficiency is required, a more complicated variant known as the tree is used. B∗ Consider again the linear index. When the collection of records will not change, a linear index provides an extremely efficient way to search. The problem is how to handle those pesky inserts and deletes. We could try to keep the core idea of storing a sorted array- based list, but make it more flexible by breaking the list into manageable chunks that are more easily updated. How might we do that? First, we need to decide how big the chunks should be. Since the data are on disk, it seems reasonable to store a chunk that is the size of a disk block, or a small multiple of the disk block size. If the next record to be inserted belongs to a chunk that hasn’t filled its block",
        "the chunks should be. Since the data are on disk, it seems reasonable to store a chunk that is the size of a disk block, or a small multiple of the disk block size. If the next record to be inserted belongs to a chunk that hasn’t filled its block then we can just insert it there. The fact that this might cause other records in that chunk to move a little bit in the array is not important, since this does not cause any extra disk accesses so long as we move data within that chunk. But what if the chunk fills up the entire block that contains it? We could just split it in half. What if we want to delete a record? We could just take the deleted record out of the chunk, but we might not want a lot of near-empty chunks. So we could put adjacent chunks together if they have only a small amount of data between them. Or we could shuffle data between adjacent chunks that together contain more data. The big problem would be how to find the desired chunk when processing a record with a given key. Perhaps some sort of tree-like structure could be used to locate the appropriate chunk. These ideas are exactly what motivate the array-based list, where the list is broken into chunks. B+ tree. The B+ tree is essentially a mechanism for managing a sorted The most significant difference between the B+ tree and the BST or the standard B-tree is that the B+ tree stores records only at the leaf nodes. Internal nodes store key values, but these are used solely as placeholders to guide the search. This means that internal nodes B+ are significantly different in structure from leaf nodes. Internal nodes store keys to guide the search, associating each key with a pointer tree to a child m tree node. Leaf nodes store actual records, or else keys and pointers to actual records in a separate disk file if the is being used purely as an index. Depending on the size of a record as compared to the size of a key, a leaf node in a might have enough room to store more or less than records. The requirement is simply that the leaf nodes store enough records to tree of order B+ B+ m remain at least half full. The leaf nodes of a B+ tree are normally linked together to form a doubly linked list. Thus, the entire collection of records can be traversed in sorted order by visiting all the leaf nodes on the linked list. Here is a Java-like pseudocode representation for the tree node interface. Leaf node and internal node subclasses would implement this interface. B+ /** Interface for B+ Tree nodes */ public interface BPNode<Key,E> { public boolean isLeaf(); public int numrecs(); public Key[] keys(); } An important implementation detail to note is that while Figure 12.6.1 shows internal nodes containing three keys and four pointers,",
        "and internal node subclasses would implement this interface. B+ /** Interface for B+ Tree nodes */ public interface BPNode<Key,E> { public boolean isLeaf(); public int numrecs(); public Key[] keys(); } An important implementation detail to note is that while Figure 12.6.1 shows internal nodes containing three keys and four pointers, class tree as it is traditionally drawn. To simplify BPNode is slightly different in that it stores key/pointer pairs. Figure 12.6.1 shows the implementation in practice, nodes really do associate a key with each pointer. Each internal node should be assumed to hold in the B+ https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 2/9 3/12/25, 1:18 PM 12.6. B-Trees — CS3 Data Structures & Algorithms leftmost position an additional key that is less than or equal to any possible key value in the node’s leftmost subtree. B+ tree implementations typically store an additional dummy record in the leftmost leaf node whose key value is less than any legal key value. Let’s see in some detail how the simplest B+ tree works. This would be the “ 2 − 3+ tree”, or a B+ tree of order 3. 1 / 28 << < > >> Example 2-3+ Tree Visualization: Insert Figure 12.6.2: An example of building a 2 − 3+ tree Next, let’s see how to search. 1 / 10 << < > >> Example 2-3+ Tree Visualization: Search 46 65 33 52 71 15 J 22 X 33 O 46 H 47 L 52 B 65 S 71 W 89 M Figure 12.6.3: An example of searching a 2 − 3+ tree Finally, let’s see an example of deleting from the 2 − 3+ tree 1 / 33 https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 3/9 3/12/25, 1:18 PM 12.6. B-Trees — CS3 Data Structures & Algorithms << < > >> Example 2-3+ Tree Visualization: Delete 46 65 22 51 71 Figure 12.6.4: An example of deleting from a 2 − 3+ tree Now, let’s extend these ideas to a B+ tree of higher order. B+ trees are exceptionally good for range queries. Once the first record in the range has been found, the rest of the records with keys in the range can be accessed by sequential processing of the remaining records in the first node, and then continuing down the linked list of leaf nodes as far as necessary. Figure illustrates the B+ tree. 1 / 10 << < > >> Example B+ Tree Visualization: Search in a tree of degree 4 77 25 40 98 10 S 18 E 25 T 39 F 40 Q 55 F 77 A 89 B 98 A 127 V Figure 12.6.5: An example of search in a B+ tree of order four. Internal nodes must store between two and four children. Search in a B+ tree is nearly identical to search in a regular B-tree, except that the search must always continue to the proper leaf node. Even if the search-key value is found in an internal node, this is only a placeholder and does not provide access to the actual record. Here",
        "a B+ tree is nearly identical to search in a regular B-tree, except that the search must always continue to the proper leaf node. Even if the search-key value is found in an internal node, this is only a placeholder and does not provide access to the actual record. Here is a pseudocode sketch of the B+ tree search algorithm. private E findhelp(BPNode<Key,E> rt, Key k) { int currec = binaryle(rt.keys(), rt.numrecs(), k); if (rt.isLeaf()) { if ((((BPLeaf<Key,E>)rt).keys())[currec] == k) { https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 4/9 3/12/25, 1:18 PM 12.6. B-Trees — CS3 Data Structures & Algorithms return ((BPLeaf<Key,E>)rt).recs(currec); } else { return null; } } else{ return findhelp(((BPInternal<Key,E>)rt).pointers(currec), k); } } B+ tree insertion is similar to B-tree insertion. First, the leaf L that should contain the record is found. If L is not full, then the new record is added, and no other is already full, split it in two (dividing the records evenly among the two nodes) and promote a copy of the least-valued key in the newly formed right node. As with the 2-3 tree, promotion might cause the parent to tree nodes are affected. If L B+ split in turn, perhaps eventually leading to splitting the root and causing the nodes at equal depth. Figure illustrates the insertion process through several examples. B+ tree to gain a new level. B+ tree insertion keeps all leaf 1 / 42 << < > >> Example B+ Tree Visualization: Insert into a tree of degree 4 Figure 12.6.6: An example of building a B+ tree of order four. Here is a a Java-like pseudocode sketch of the B+ tree insert algorithm. private BPNode<Key,E> inserthelp(BPNode<Key,E> rt, Key k, E e) { BPNode<Key,E> retval; if (rt.isLeaf()) { // At leaf node: insert here return ((BPLeaf<Key,E>)rt).add(k, e); } // Add to internal node int currec = binaryle(rt.keys(), rt.numrecs(), k); BPNode<Key,E> temp = inserthelp( ((BPInternal<Key,E>)root).pointers(currec), k, e); if (temp != ((BPInternal<Key,E>)rt).pointers(currec)) { return ((BPInternal<Key,E>)rt). add((BPInternal<Key,E>)temp); } else{ return rt; } } https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 5/9 3/12/25, 1:18 PM 12.6. B-Trees — CS3 Data Structures & Algorithms Here is an exercise to see if you get the basic idea of B+ tree insertion. Instructions: B+ Tree Insertion In this exercise your job is to insert the values from the stack to the B+ tree. Search for the leaf node where the topmost value of the stack should be inserted, and click on that node. The exercise will take care of the rest. Continue this procedure until you have inserted all the values in the stack. Undo Reset Model Answer Grade 8598727588243282159173606364 90 18 31 49 To delete record L leaving still at least half full. This is demonstrated by Figure . R from the B+ tree, first locate the leaf L that contains R . If L is more than half full, then we need only remove R , 1 / 23 << < > >> Example B+ Tree Visualization: Delete from a tree of degree 4 58 12 44 67 https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 6/9 3/12/25, 1:18 PM 12.6. B-Trees",
        "the leaf L that contains R . If L is more than half full, then we need only remove R , 1 / 23 << < > >> Example B+ Tree Visualization: Delete from a tree of degree 4 58 12 44 67 https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 6/9 3/12/25, 1:18 PM 12.6. B-Trees — CS3 Data Structures & Algorithms 5 F 10 S 12 V 27 T 44 Q 48 E 58 A 60 F 67 A 88 B Figure 12.6.7: An example of deletion in a B+ tree of order four. If deleting a record reduces the number of records in the node below the minimum threshold (called an underflow), then we must do something to keep the node sufficiently full. The first choice is to look at the node’s adjacent siblings to determine if they have a spare record that can be used to fill the gap. If so, then enough records are transferred from the sibling so that both nodes have about the same number of records. This is done so as to delay as long as possible the next time when a delete causes this node to underflow again. This process might require that the parent node has its placeholder key value revised to reflect the true first key value in each node. If neither sibling can lend a record to the under-full node (call it N ), then N must give its records to a sibling and be removed from the tree. There is certainly room to do this, because the sibling is at most half full (remember that it had no records to contribute to the has become less than half full because it is under-flowing. This merge process combines two subtrees of the current node), and N parent, which might cause it to underflow in turn. If the last two children of the root merge together, then the tree loses a level. Here is a Java-like pseudocode for the B+ tree delete algorithm. /** Delete a record with the given key value, and return true if the root underflows */ private boolean removehelp(BPNode<Key,E> rt, Key k) { int currec = binaryle(rt.keys(), rt.numrecs(), k); if (rt.isLeaf()) { if (((BPLeaf<Key,E>)rt).keys()[currec] == k) { return ((BPLeaf<Key,E>)rt).delete(currec); } else { return false; } } else{ // Process internal node if (removehelp(((BPInternal<Key,E>)rt).pointers(currec), k)) { // Child will merge if necessary return ((BPInternal<Key,E>)rt).underflow(currec); } else { return false; } } } The B+ tree requires that all nodes be at least half full (except for the root). Thus, the storage utilization must be at least 50%. This is satisfactory for many implementations, but note that keeping nodes fuller will result both in less space required (because there is less empty space in the disk file) and in more efficient processing (fewer blocks on average will be read into memory because the amount of information in each block is greater). Because B-trees have become so popular, many algorithm designers have tried to improve B-tree performance. One method for doing so is to use",
        "the disk file) and in more efficient processing (fewer blocks on average will be read into memory because the amount of information in each block is greater). Because B-trees have become so popular, many algorithm designers have tried to improve B-tree performance. One method for doing so is to use the for the rules used to split and merge nodes. Instead of splitting a node in half when it overflows, the tree variant known as the tree. The tree is identical to the tree, except tree gives some records to its B∗ B∗ B∗ B+ B+ neighboring sibling, if possible. If the sibling is also full, then these two nodes split into three. Similarly, when a node underflows, it is combined with its two siblings, and the total reduced to two nodes. Thus, the nodes are always at least two thirds full. [1] Finally, here is an example of building a B+ Tree of order five. You can compare this to the example above of building a tree of order four with the same records. 1 / 33 << < > >> Example B+ Tree Visualization: Insert into a tree of degree 5 https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 7/9 3/12/25, 1:18 PM 12.6. B-Trees — CS3 Data Structures & Algorithms Figure 12.6.8: An example of building a B+ tree of degree 5 Click here for a visualization that will let you construct and interact with a University of San Francisco as part of his Data Structure Visualizations package. B+ tree. This visualization was written by David Galles of the [1] This concept can be extended further if higher space utilization is required. However, the update routines become much more complicated. I once worked on a project where we implemented 3-for-4 node split and merge routines. This gave better performance tree. However, the spitting and merging routines were so complicated that even than the 2-for-3 node split and merge routines of the B∗ their author could no longer understand them once they were completed! 12.6.1.2. B-Tree Analysis The asymptotic cost of search, insertion, and deletion of records from B-trees, B+ trees, and B∗ trees is Θ(log n) where n is the total number of records in the tree. However, the base of the log is the (average) branching factor of the tree. Typical database applications use extremely high branching factors, perhaps 100 or more. Thus, in practice the B-tree and its variants are extremely shallow. As an illustration, consider a B+ tree of order 100 and leaf nodes that contain up to 100 records. A B- B+ tree with height one (that is, just a single leaf node) can have at most 100 records. A B+ tree with height two (a root internal node whose children are leaves) must have at least 100 records (2 leaves with 50 records each). It has at most 10,000 records (100 leaves with 100 records each). A tree with height three must have at least 5000 records (two second-level nodes with 50 children containing 50 records each) and at",
        "children are leaves) must have at least 100 records (2 leaves with 50 records each). It has at most 10,000 records (100 leaves with 100 records each). A tree with height three must have at least 5000 records (two second-level nodes with 50 children containing 50 records each) and at most one B+ million records (100 second-level nodes with 100 full children each). A most 100 million records. Thus, it would require an extremely large database to generate a B+ tree of more than height four. B+ tree with height four must have at least 250,000 records and at The B+ tree split and insert rules guarantee that every node (except perhaps the root) is at least half full. So they are on average about 3/4 full. But the internal nodes are purely overhead, since the keys stored there are used only by the tree to direct search, rather than store actual data. Does this overhead amount to a significant use of space? No, because once again the high fan-out rate of the tree of its nodes as internal nodes. This structure means that the vast majority of nodes are leaf nodes. A K-ary tree has approximately 1/75 tree of order 100 probably only about means that while half of a full binary tree’s nodes are internal nodes, in a internal nodes. This means that the overhead associated with internal nodes is very low. of its nodes are 1/K B+ We can reduce the number of disk fetches required for the B-tree even more by using the following methods. First, the upper levels of the tree can be stored in main memory at all times. Because the tree branches so quickly, the top two levels (levels 0 and 1) require relatively little space. If the B-tree is only height four, then at most two disk fetches (internal nodes at level two and leaves at level three) are required to reach the pointer to any given record. A buffer pool could be used to manage nodes of the B-tree. Several nodes of the tree would typically be in main memory at one time. The most straightforward approach is to use a standard method such as LRU to do node replacement. However, sometimes it might be desirable to “lock” certain nodes such as the root into the buffer pool. In general, if the buffer pool is even of modest size (say at least twice the depth of the tree), no special techniques for node replacement will be required because the upper-level nodes will naturally be accessed frequently. https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 8/9 3/12/25, 1:18 PM 12.6. B-Trees — CS3 Data Structures & Algorithms https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 9/9"
    ],
    "C12-bst.pdf": [
        "Chapter 12: Binary Search Trees A binary search tree is a binary tree with a special property called the BST-property, which is given as follows: ? For all nodes x and y, if y belongs to the left subtree of x, then the key at y is less than the key at x, and if y belongs to the right subtree of x, then the key at y is greater than the key at x. We will assume that the keys of a BST are pairwise distinct. Each node has the following attributes: (cid:15) p, left, and right, which are pointers to the parent, the left child, and the right child, respectively, and (cid:15) key, which is key stored at the node. 1 An example 7 4 12 2 6 9 19 3 5 8 11 15 20 2 Traversal of the Nodes in a BST By \\traversal\" we mean visiting all the nodes in a graph. Traversal strategies can be speci(cid:12)ed by the ordering of the three objects to visit: the current node, the left subtree, and the right subtree. We assume the the left subtree always comes before the right subtree. Then there are three strategies. 1. Inorder. The ordering is: the left subtree, the current node, the right subtree. 2. Preorder. The ordering is: the current node, the left subtree, the right subtree. 3. Postorder. The ordering is: the left subtree, the right subtree, the current node. 3 Inorder Traversal Pseudocode This recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree. While doing traversal it prints out the key of each node that is visited. Inorder-Walk(x) if x = nil then return 1: 2: Inorder-Walk(left[x]) 3: Print key[x] 4: Inorder-Walk(right[x]) We can write a similar pseudocode for preorder and postorder. 4 2 1 3 1 3 2 3 1 2 inorder preorder postorder 7 4 12 2 6 9 19 3 5 8 11 15 20 What is the outcome of inorder traversal on this BST? How about postorder traversal and preorder traversal? 5 Inorder traversal gives: 2, 3, 4, 5, 6, 7, 8 , 9, 11, 12, 15, 19, 20. Preorder traversal gives: 7, 4, 2, 3, 6, 5, 12, 9, 8, 11, 19, 15, 20. Postorder traversal gives: 3, 2, 5, 6, 4, 8, 11, 9, 15, 20, 19, 12, 7. So, inorder travel on a BST (cid:12)nds the keys in nondecreasing order! 6 Operations on BST 1. Searching for a key We assume that a key and the subtree in which the key is searched for are given as an input. We’ll take the full advantage of the BST-property. Suppose we are at a node. If the node has the key that is being searched for, then the search is over. Otherwise, the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for. If the former is the case,",
        "If the node has the key that is being searched for, then the search is over. Otherwise, the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for. If the former is the case, then by the BST property, all the keys in th left subtree are strictly less than the key that is searched for. That means that we do not need to search in the left subtree. Thus, we will examine only the right subtree. If the latter is the case, by symmetry we will examine only the right subtree. 7 Algorithm Here k is the key that is searched for and x is the start node. BST-Search(x, k) 1: y x 2: while y 6= nil do if key[y] = k then return y 3: else if key[y] < k then y right[y] 4: else y left[y] 5: 6: return (\\NOT FOUND\") 8 An Example 7 search for 8 4 11 2 6 9 13 NIL What is the running time of search? 9 2. The Maximum and the Minimum To (cid:12)nd the minimum identify the leftmost node, i.e. the farthest node you can reach by following only left branches. To (cid:12)nd the maximum identify the rightmost node, i.e. the farthest node you can reach by following only right branches. BST-Minimum(x) if x = nil then return (\\Empty Tree\") 1: 2: y x 3: while left[y] 6= nil do y left[y] 4: return (key[y]) BST-Maximum(x) if x = nil then return (\\Empty Tree\") 1: 2: y x 3: while right[y] 6= nil do y right[y] 4: return (key[y]) 10 3. Insertion Suppose that we need to insert a node z such that k = key[z]. Using binary search we (cid:12)nd a nil such that replacing it by z does not break the BST-property. 11 if x = nil then return \\Error\" BST-Insert(x, z, k) 1: 2: y x 3: while true do f if key[y] < k 4: then z left[y] 5: else z right[y] 6: if z = nil break 7: 8: g 9: if key[y] > k then left[y] z 10: else right[p[y]] z 12 4. The Successor and The Predecessor The successor (respectively, the predecessor) of a key k in a search tree is the smallest (respectively, the largest) key that belongs to the tree and that is strictly greater than (respectively, less than) k. The idea for (cid:12)nding the successor of a given node x. (cid:15) If x has the right child, then the successor is the minimum in the right subtree of x. (cid:15) Otherwise, the successor is the parent of the farthest node that can be reached from x by following only right branches backward. 13 An Example 7 4 23 12 25 2 6 9 19 3 5 8 11 15 20 14 Algorithm BST-Successor(x) if right[x] 6= nil then 1: 2: f 3: 4: 5: else 6: f 7: 8: 9: y right[x]",
        "be reached from x by following only right branches backward. 13 An Example 7 4 23 12 25 2 6 9 19 3 5 8 11 15 20 14 Algorithm BST-Successor(x) if right[x] 6= nil then 1: 2: f 3: 4: 5: else 6: f 7: 8: 9: y right[x] while left[y] 6= nil do y left[y] return (y) g y x while right[p[x]] = x do y p[x] if p[x] 6= nil then return (p[x]) else return (\\NO SUCCESSOR\") g 15 The predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged. For which node is the successor unde(cid:12)ned? What is the running time of the successor algorithm? 16 5. Deletion Suppose we want to delete a node z. 1. If z has no children, then we will just replace z by nil. 2. If z has only one child, then we will promote the unique child to z’s place. 3. If z has two children, then we will identify z’s successor. Call it y. The successor y either is a leaf or has only the right child. Promote y to z’s place. Treat the loss of y using one of the above two solutions. 17 8 11 9 13 7 10 1 2 8 11 9 13 3 7 10 2 8 11 9 13 7 10 1 2 5 3 5 4 5 3 5 3 5 3 5 3 1 2 1 2 1 2 6 4 6 4 6 4 11 9 13 10 8 8 6 4 11 6 9 13 7 10 9 11 10 13 6 4 18 Algorithm This algorithm deletes z from BST T . BST-Delete(T , z) if left[z] = nil or right[z] = nil 1: 2: then y z 3: else y BST-Successor(z) 4: (cid:3) y is the node that’s actually removed. 5: (cid:3) Here y does not have two children. if left[y] 6= nil 6: 7: then x left[y] 8: else x right[y] 9: (cid:3) x is the node that’s moving to y’s position. if x 6= nil then p[x] p[y] 10: 11: (cid:3) p[x] is reset If x isn’t NIL. 12: (cid:3) Resetting is unnecessary if x is NIL. 19 Algorithm (cont’d) then left[p[y]] x if p[y] = nil then root[T ] x 13: 14: (cid:3) If y is the root, then x becomes the root. 15: (cid:3) Otherwise, do the following. 16: else if y = left[p[y]] 17: 18: (cid:3) If y is the left child of its parent, then 19: (cid:3) Set the parent’s left child to x. else right[p[y]] x 20: 21: (cid:3) If y is the right child of its parent, then 22: (cid:3) Set the parent’s right child to x. 23: 24: f 25: 27: return (y) key[z] key[y] Move other data from y to z g if y 6= z then 20 Summary of E(cid:14)ciency Analysis Theorem A On a binary search tree of height h, Search, Minimum,",
        "parent, then 22: (cid:3) Set the parent’s right child to x. 23: 24: f 25: 27: return (y) key[z] key[y] Move other data from y to z g if y 6= z then 20 Summary of E(cid:14)ciency Analysis Theorem A On a binary search tree of height h, Search, Minimum, Maximum, Successor, Predecessor, Insert, and Delete can be made to run in O(h) time. 21 Randomly built BST Suppose that we insert n distinct keys into an initially empty tree. Assuming that the n! permutations are equally likely to occur, what is the average height of the tree? To study this question we consider the process of constructing a tree T by inserting in order randomly selected n distinct keys to an initially empty tree. Here the actually values of the keys do not matter. What matters is the position of the inserted key in the n keys. 22 The Process of Construction So, we will view the process as follows: A key x from the keys is selected uniformly at random and is inserted to the tree. Then all the other keys are inserted. Here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree. Thus, the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree. 23 Random Variables n = number of keys Xn = height of the tree of n keys Yn = 2Xn. We want an upper bound on E[Yn]. For n (cid:21) 2, we have E[Yn] = 1 n n 0 X i=1 @ 2E[maxfYi(cid:0)1; Yn(cid:0)ig]1 A : E[maxfYi(cid:0)1; Yn(cid:0)ig] (cid:20) E[Yi(cid:0)1 + Yn(cid:0)i] (cid:20) E[Yi(cid:0)1] + E[Yn(cid:0)i] Collecting terms: E[Yn] (cid:20) 4 n n(cid:0)1 X i=1 E[Yi]: 24 Analysis We claim that for all n (cid:21) 1 E[Yn] (cid:20) 1 4(cid:16) We prove this by induction on n. n+3 3 (cid:17). Base case: E[Y1] = 20 = 1. Induction step: We have E[Yn] (cid:20) 4 n n(cid:0)1 X i=1 E[Yi] Using the fact that n(cid:0)1 (cid:16) X i=0 i + 3 3 (cid:17) = (cid:16) n + 3 4 (cid:17) E[Yn] (cid:20) 4 n (cid:1) 1 4 (cid:1) (cid:16) n + 3 4 (cid:17) E[Yn] (cid:20) 1 4 (cid:1) (cid:16) n + 3 3 (cid:17) 25 Jensen’s inequality A function f is convex if for all x and y, x < y, and for all (cid:21), 0 (cid:20) (cid:21) (cid:20) 1, f ((cid:21)x + (1 (cid:0) (cid:21))y) (cid:20) (cid:21)f (x) + (1 (cid:0) (cid:21))f (y) Jensen’s inequality states that for all random variables X and for all convex function f f (E[X]) (cid:20) E[f (X)]: Let this X be Xn and f (x) = 2x. Then E[f (X)] = E[Yn]. So, we have 2E[Xn] (cid:20) 1 4(cid:16) n + 3 3 (cid:17): The right-hand side is at most (n + 3)3. By taking the log of both sides, we have E[Xn] = O(log",
        "E[f (X)]: Let this X be Xn and f (x) = 2x. Then E[f (X)] = E[Yn]. So, we have 2E[Xn] (cid:20) 1 4(cid:16) n + 3 3 (cid:17): The right-hand side is at most (n + 3)3. By taking the log of both sides, we have E[Xn] = O(log n): Thus the average height of a randomly build BST is O(log n). 26"
    ],
    "Shreesh transcribed notes.pdf": [
        "AVL Tree: Approximately balanced binary search tree • For any AVL Tree the height of its left sub tree and right sub tree do not vary by more than one | h(LST) - h(RST)| <= 1 (The height is the number of levels from a chosen node on either the left or right side) • The node from which a new number is added is called the node of imbalance • 4 Cases or Imbalance (Determined by where exactly the insertion is, and which node becomes “deeper” than the rest) Memory Hierarchy: DB systems aim to minimize HDD/SDD accesses • SDD/HDD use lots of storage and are slow • Hard drive stores in “blocks”: o If we have a 2048 byte block and we want to access a 64 bit it (8 bytes), we stlll must search the whole 2048 byte block • Consider a sorted array of 128 ints (1024 bytes) o If its all in one block, binary search is much faster than even searching for 1 out of 2 ints where the ints are in 2 different blocks (less disk accesses) o Remember we don’t get too much control, but the general idea is to reduce the number of different disk accesses B+ Tree o Minimizes the height of the tree • Node structure of B+ Tree: o Maximum of 2 children per node o All nodes except the root must be at least half full o Leaves are stored as DLL Internal nodes only store keys and pointers to other values • • Leaf nodes can access values too o The more things we put into nodes, the less space there is for keys, resulting o in a deeper tree, which we don’t want IIn B trees, nodes store keys and values, but in B+ trees, only leaf nodes can access values • Splitting a B+ tree o Tree only gets deeper once the root node needs to split o Smallest value of a new leaf node gets copied to the parent node ▪ New node always contains the greater of the values we are splitting o When you split an internal node, greatest value og the new node gets moved up, not copied (only copied when leaf splits) NoSQL and Alternative DBs • NoSQL means Not Only SQL, refers to non-relational databases that store data in a non-tabular format • CAP Theorem o Consistency: Every user of the database has an identical view of the data at any given instant o Availability: In the event of a failure, the database system remains operational o Partition Tolerance: Database continues to function even when there is a communication break between nodes • If htere is a break in communication between two nodes, one can either o Allow nodes to get out of sync (giving up consistency), or o Consider the system to go “down” (giving up availability) • All options: o CA: Data is consistent and available, but if there is a partition, data will be out",
        "in communication between two nodes, one can either o Allow nodes to get out of sync (giving up consistency), or o Consider the system to go “down” (giving up availability) • All options: o CA: Data is consistent and available, but if there is a partition, data will be out of sync o CP: Data is consistent, and partition tolerance is maintained by making the system unavailable when a node goes down o AP: Nodes remain online even if they can’t communicate with each other, but no guarantee that the nodes will all have the same data at the same times RDBMS like MySQL use CA NoSQL like Redis and MongoDB use CP Key Value Stores • Comparatively simpler than tabular RDBMS • Lends itself to easy CRUD operations and API creation • KV stores are usually deployed as in-memory Databases • Getting a value is typically O(1) because hash tables or a similar structure is under the hood"
    ]
}