from src.utils.embedding_model import EmbeddingModel
from src.utils.db_model import DBModel
from src.utils.llm_model import LLMModel


class RAG:
    def __init__(self, embedding_model: EmbeddingModel, vector_db: DBModel, llm: LLMModel):
        """
        Initialize a RAG pipeline with the given models.
        :param embedding_model: Embedding model to generate embeddings for the query.
        :param vector_db: Vector database to retrieve context for the query.
        :param llm: Language model to generate responses for the query.
        """
        self.embedding_model = embedding_model
        self.vector_db = vector_db
        self.llm = llm

    def run(self, query: str, top_k: int = 1) -> str:
        """
        Run the RAG pipeline with the given query and return the response.
        :param query: Query from the user.
        :param top_k: Number of documents to retrieve from the database.
        :return: Response generated by the pipeline.
        """
        # 1. Generate embedding for the query
        query_embedding = self.embedding_model.generate_embeddings(query)

        # 2. Query the database to retrieve context
        retrieved_docs = self.vector_db.query_db(query_embedding, top_k)
        context = " ".join(retrieved_docs)

        # 3. Formulate the full prompt for the LLM
        prompt = f"Context: {context}\n\nQuestion: {query}\nAnswer:"

        # 4. Generate and return the response
        return self.llm.generate_response(prompt)
