from typing import Any
from numpy import ndarray
from src.utils.embedding_model import EmbeddingModel
from src.utils.db_model import DBModel
from src.utils.llm_model import LLMModel
import numpy as np


class RAG:
    def __init__(self, embedding_model: EmbeddingModel, vector_db: DBModel, llm: LLMModel):
        """
        Initialize a RAG pipeline with the given models.
        :param embedding_model: Embedding model to generate embeddings for the query.
        :param vector_db: Vector database to retrieve context for the query.
        :param llm: Language model to generate responses for the query.
        """
        self.embedding_model = embedding_model
        self.vector_db = vector_db
        self.llm = llm

    def run(self, query: str, base_prompt: str,top_k: int = 1) -> tuple[
        str, dict[str, ndarray | str | list[list[dict]] | Any]]:
        """
        Run the RAG pipeline with the given query and return the response.
        :param query: Query from the user.
        :param top_k: Number of documents to retrieve from the database.
        :param base_prompt: Base prompt for the LLM.
        :return:
            response: Response generated by the LLM.
            results: List of all metadata retrieved from the database.
        """
        # 1. Generate embedding for the query
        query_embedding = self.embedding_model.generate_embeddings(query)

        # 2. Query the database to retrieve context
        context, results = self.vector_db.query_db(query_embedding, top_k)
        print("Context that the model is using: ", context)
        llm_context = " ".join(context)

        # 3. Formulate the full prompt for the LLM
        prompt = base_prompt + f"Context: {llm_context}\n\nQuestion: {query}\nAnswer:"

        llm_response = self.llm.generate_response(prompt)
        response_embedding = self.embedding_model.generate_embeddings(llm_response)

        # Create the metadata that RAG will return
        query_metadata = {
            "query": query,
            "query_embedding": query_embedding,
            "llm_context": llm_context,
            "prompt": prompt,
            "db_metadata": [results],
            "response_similarity_to_query": self.cosine_similarity(response_embedding, query_embedding)
        }

        # 4. Generate and return the response
        return llm_response, query_metadata

    @staticmethod
    def cosine_similarity(embedding1, embedding2):
        return np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))
