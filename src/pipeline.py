from typing import Any
from numpy import ndarray
from src.utils.embedding_model import EmbeddingModel
from src.utils.db_model import DBModel
from src.utils.llm_model import LLMModel
import numpy as np


class RAG:
    def __init__(self, embedding_model: EmbeddingModel, vector_db: DBModel, llm: LLMModel):
        """
        Initialize a RAG pipeline with the given models.
        :param embedding_model: Embedding model to generate embeddings for the query.
        :param vector_db: Vector database to retrieve context for the query.
        :param llm: Language model to generate responses for the query.
        """
        self.embedding_model = embedding_model
        self.vector_db = vector_db
        self.llm = llm

    def run(self, query: str, base_prompt: str, use_context=True, top_k: int = 1) -> tuple[
        str, dict[str, ndarray | str | list[list[dict]] | Any]]:
        """
        Run the RAG pipeline with the given query and return the response.
        :param query: Query from the user.
        :param top_k: Number of documents to retrieve from the database.
        :param base_prompt: Base prompt for the LLM.
        :return:
            response: Response generated by the LLM.
            results: List of all metadata retrieved from the database.
        """
        # 1. Generate embedding for the query
        query_embedding = self.embedding_model.generate_embeddings(query)

        # If told not to use context, generate response directly
        if not use_context:
            # If no context is needed, generate response directly
            prompt = base_prompt + f"\n\nQuestion: {query}\nAnswer:"
            llm_response = self.llm.generate_response(prompt)

        # Else, query DB and get context
        else:
            context, results = self.vector_db.query_db(query_embedding, top_k)
            print("Context that the model is using: ", context)
            llm_context = " ".join(context)

            # 3. Formulate the full prompt for the LLM
            prompt = base_prompt + f"Context: {llm_context}\n\nQuestion: {query}\nAnswer:"

            llm_response = self.llm.generate_response(prompt)

        # Generate embedding for response (for similarity to query)
        response_embedding = self.embedding_model.generate_embeddings(llm_response)

        # Create the metadata that RAG will return
        query_metadata = {
            "query": query,
            "query_embedding": query_embedding,
            "llm_context": llm_context if use_context else None,
            "prompt": prompt,
            "db_metadata": [results] if use_context else None,
            "response_similarity_to_query": self.cosine_similarity(response_embedding, query_embedding)
        }

        # 4. Generate and return the response
        return llm_response, query_metadata

    @staticmethod
    def cosine_similarity(embedding1: np.ndarray, embedding2: np.ndarray) -> float | np.nan:
        """
        Calculate the cosine similarity between two embeddings.
        :param embedding1: First embedding
        :param embedding2: Second embedding
        :return: Cosine similarity score between the two embeddings. Returns NaN if an error occurs.
        """
        try:
            return np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))
        except Exception as e:
            print("Error calculating cosine similarity:", e)
            return np.nan
